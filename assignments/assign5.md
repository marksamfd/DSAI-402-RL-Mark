# Assignment 5

## Reading 

- Read Section 6.1 from the main book
- Watch this video [Temporal Difference Explained – The Key to Q-Learning, by Super Data Science](https://www.youtube.com/watch?v=yaIYa9TP780&t=1s)
- ... and this video [Temporal Difference Learning (including Q-Learning) | Reinforcement Learning Part 4, by Mutual Information](https://www.youtube.com/watch?v=AJiG3ykOxmY&t=953s)

## Implementation 

Due 11/28. 

**Replicating Monte Carlo and Temporal Difference Learning Visualization**

In the video “Temporal Difference Learning (including Q-Learning) | Reinforcement Learning Part 4” by Mutual Information, between minutes 3 and 6, there is a very nice animation of the episodes in the lower panel, accumulated rewards in the upper panel, and the evolving value function on the right lower panel.
Your task is to replicate a similar demonstration with the following requirements:
- Generate multiple episodes of state and reward sequences resembling those shown in the video (they don’t need to be exact replicas, but should be similar in spirit)
- Implement the Monte Carlo algorithm to update the value function based on these episodes, and visualize the updated values in a right-side panel
- Implement one-step Temporal Difference (TD) learning as presented in the lecture, showing how the value function is updated incrementally as the episode progresses

You can easily ask ChatGPT or other tools to generate the solutions for you, BUT the true learning experience lies in working through the implementation yourself. Completing this assignment, especially the MC and TD algorithms, independently, except for the visualization aspect, will deepen your understanding of the course material and solidify your grasp on reinforcement learning concepts. Believe it or not you will master RL soon.Embrace the challenge: it may be tough, but it is rewarding and enjoyable.
